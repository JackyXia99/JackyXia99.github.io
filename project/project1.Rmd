---
title: "Project1"
author: "Jacky"
date: "4/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```
    
#### 1. Tidying: Rearranging Wide/Long (10 pts)

- Tidy the datasets (using the `tidyr` functions `pivot_longer`/`gather` and/or `pivot_wider`/`spread`) 
- If you data sets are already tidy, be sure to use those functions somewhere else in your project (e.g., for rearranging summary statistics)
- Document the process (describe in words what was done)

```{R}
library("readxl")
library(dplyr)
ou<-read_excel("gen1ou.xlsx")
ou<-ou%>%rename(Usage=`Usage %`,RawPer=`%...5`,RealPer=`%...7`)
head(ou)
glimpse(ou)
gen1<-read.csv("FirstGenPokemon.csv", header = TRUE)
head(gen1)
glimpse(gen1)
```

*I have chosen Pokemon as my topic for this project since it is something that is very close to me as well as it having a surprisingly deep foundation in numbers and statistics. I used two datasets for this project. Although Pokemon has 8 generations worth of material, when considering the available online datasets, I have chosen to focus on the first generation. The first dataset (gen1ou) was taken off of smogon.com, the place of online, competitive Pokemon battling. It comprises of the usage statistics of each generation 1 Pokemon during the month of Marth 2021 for the online generation 1 OU metagame. The second dataset (FirstGenPokemon) was taken from kaggle uploaded by Alejandro Ojeda (https://www.kaggle.com/dizzypanda/gen-1-pokemon). This dataset contains all the essential information of all the generation 1 pokemon. Moving onto part one of the project, I first loaded in the necessary packages. Afterwards, I familiarized myself with each dataset using the head() and glimpse() commands. I noticed that the ou dataset had headers that were named improperly so I went ahead and changed those into ones that would work. Since both datasets were tidy to start off with, I will untidy them and then tidy them in part 2 of the project, after I join the datasets together.*
    
#### 2. Joining/Merging (10 pts)

- Join your datasets into one using a `dplyr` join function
- If you have multiple observations on the joining variable in either dataset, fix this by collapsing via summarize
- Discuss the process in words, including why you chose the join you did
- Discuss which cases were dropped, if any, and potential problems with this
    
```{R}
library(tidyverse)
joined<-ou%>%inner_join(gen1, by=c("Pokemon"="Name"))%>%select(-c(4:9,11,17,25:41))
glimpse(joined)
untidy<-joined%>%pivot_wider(names_from="Type1", values_from="Pokemon")
joined%>%pivot_wider(names_from="Type1", values_from="Pokemon")%>%head()
tidy<-untidy%>%pivot_longer(c(15:28), names_to="Type1", values_to="Pokemon", values_drop_na=T)
untidy%>%pivot_longer(c(15:28), names_to="Type1", values_to="Pokemon", values_drop_na=T)%>%head()

```

*When joining the two datasets, I used the Pokemon names column since both datasets had it as an identifier. Ideally, left, right, and inner join would all work since each dataset should have their headers and exactly 151 pokemon. However, the ou dataset contains pokemon usable in the OU metagame. Sadly, this does not include Mewtwo or Mew, so this dataset only had 149 pokemon. To make the OU dataset more stubborn, there were an additional 6 observations of NAs for some reason at the end of the dataset, making the total observation number 155. On the otherhand, the gen1ou dataset had the ideal 151 observations. Since the ou dataset was missing Mewtwo and Mew and had some additional NAs, I used the inner_join function to join the two datasets so that both would have a matching number of observations/Pokemon. Moving onto the code, I first loaded up the tidyverse package for later use. Then, I joined the gen1ou dataset to the ou dataset so that I didn't have to scroll all the way to the right to view the ou dataset's data once joined. Additionally, from part 1 of the project, this is where I demonstrated untidying the data and then tidying it up again. THe joined dataset was untidyed via pivot_wider, which takes the cells in the Type1 column and makes each cell into their own column with values filled in from the Pokemon column. To tidy the dataset up again, I used pivot_longer to take column names 15-28, take away the NAs, make them into one column, and placed the values back into the Pokemon column *

#### 3. Wrangling (40 pts)

- Use all six core `dplyr` functions in the service of generating summary tables/statistics (12 pts)
    - Use mutate at least once to generate a variable that is a function of at least one other variable

- Compute summary statistics for each of your variables using `summarize` alone and with `group_by` (if you have more than 10 variables, fine to just focus on 10) (20 pts)
    - Use at least 5 unique functions inside of summarize (e.g., mean, sd)
    - For at least 2, use summarize after grouping by a categorical variable. Create one by dichotomizing a numeric if necessary
    - If applicable, at least 1 of these should group by two categorical variables

- Summarize the procedure and discuss all (or the most interesting) results in no more than two paragraphs (8 pts)

```{R}
joined%>%filter(Type1=="normal")
joined%>%arrange(desc(Speed))
joined%>%select(Pokemon, Base_Total, Usage)%>%arrange(desc(Base_Total))
joined%>%mutate(BMI=Weight.kg./(Height.m.)^2)%>%select(Pokemon, BMI)%>%arrange(desc(BMI))
joined%>%summarize(mean(Male_Pct),sd(Male_Pct),mean(Female_Pct),sd(Female_Pct))
joined%>%group_by(Type1)%>%summarize(mean(Base_Total),sd(Base_Total),n(),max(Usage),first(Rank))
joined%>%group_by(Type1, Exp_Speed)%>%summarize(mean(Capt_Rate))
```

*Using all the dplyr functions was quite simple. All I had to do was pipe the joined dataset into each function and then specify the data that I wanted the function to use. From these results, there were a few interesting one. From the arrange function, it seems that pure speed does not necessarily mean a high usage statistic. Also interesting that the small monkey Mankey had a higher BMI than a rock rhino. Lastly, it seems that the ice typing had the highest average base stat total, ironic considering that it is the worst defensive type.*

4. Make visualizations (three plots)

    -  Make a correlation heatmap of your numeric variables
    -  Create at least two additional plots of your choice with ggplot that highlight some of the more interesting features of your data.
    - Each plot (besides the heatmap) should have at least three variables mapped to separate aesthetics
    - Each should use different geoms (e.g., don't do two geom_bars)
    - At least one plot should include `stat="summary"`
    - Each plot should include a supporting paragraph describing the relationships that are being visualized and any trends that are apparent
        - It is fine to include more, but limit yourself to 4. Plots should avoid being redundant! Four bad plots will get a lower grade than two good plots, all else being equal.
    - Make them pretty! Use correct labels, etc.
    
#### 4. Visualizing (30 pts)

- Create a correlation heatmap of your numeric variables the way we did in class

- Create two more effective, polished plots with ggplot

    - Each plot should map 3+ variables to aesthetics 
    - Each plot should have a title and clean labeling for all mappings
    - Change at least one default theme element and color for at least one mapping per plot
    - For at least one plot, add more tick marks (x, y, or both) than are given by default
    - For at least one plot, use the stat="summary" function
    - Supporting paragraph or two (for each plot) describing the relationships/trends that are apparent

```{R}
library(tidyverse)
cormat<-joined%>%select_if(is.numeric)%>%cor(use="pair")
cormat
tidycor <- cormat %>% as.data.frame %>% rownames_to_column("var1") %>%
pivot_longer(-1,names_to="var2",values_to="correlation")
tidycor
tidycor%>%ggplot(aes(var1,var2,fill=correlation))+
geom_tile()+
scale_fill_gradient2(low="red",mid="white",high="blue")+ #makes colors!
geom_text(aes(label=round(correlation,2)),color = "black", size = 4)+ #overlay values
theme(axis.text.x = element_text(angle = 90, hjust=1))+ #flips x-axis labels
coord_fixed() #makes it square

library(ggplot2)
plot1<-data.frame(joined)
ggplot(plot1,aes(Base_Total, Rank, color=Attack))+geom_point(stat="summary",size=7)+ggtitle("Base Stat Total Compared Against Rank Usage")
ggplot(plot1,aes(Base_Total))+geom_histogram(fill="blue")+ggtitle("Base Stat Total Counts")+scale_y_continuous(breaks=seq(0,15,1))

```

*The scatterplot shows that as the base stat total increases, the rank usage of the pokemon decreases. Which is to be expected since a Pokemon with more base stats should be stronger. Additionally, although not strong, it would also seem that as the base stat total increases and the rank decreases, the attack stat also increases, which again makes sense since a high attack stat is indicative of a strong pokemon. The histogram shows that the distribution of base stat totals across the generation 1 Pokemon is definitely not normally distributed.*

#### 5. Dimensionality Reduction (30 pts) 

- Either k-means/PAM clustering or PCA (inclusive "or") should be performed on at least three of your variables (3 is just the minimum: using more/all of them will make this much more interesting!)

    - All relevant steps discussed in class (e.g., picking number of PCs/clusters)
    - A visualization of the clusters or the first few principal components (using ggplot2)
    - Supporting paragraph or two describing results found, interpreting the clusters/PCs in terms of the original variables and observations, discussing goodness of fit or variance explained, etc.
    
```{R}
library(cluster)
clust_data<-joined%>%select(Base_Total,HP,Attack,Defense,Special,Speed)
sil_width<-vector() 
for(i in 2:10){
  kms <- kmeans(clust_data,centers=i)
  sil <- silhouette(kms$cluster,dist(clust_data))
  sil_width[i]<-mean(sil[,3])
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
pam1 <- clust_data %>% pam(k=2)
pam1
pamclust<-clust_data %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% ggplot(aes(Base_Total,HP,color=cluster)) + geom_point()
library(GGally)
joined %>% mutate(cluster=as.factor(pam1$clustering)) %>% 
  ggpairs(columns = c("Base_Total","HP","Attack","Defense","Special","Speed"), aes(color=cluster))
plot(pam1, which=2)
```
    
*Right on to the code, I first started by loading the cluster package. Using for-loops, I calculated that the optimal number of clusters by a long shot was 2 clusters. PAM clusterin with two clusters yielded a pairwise combination chart of all the input variable and a chart displaying average silhouette width. Looking at the pairwise combinations, it can be seen that there is not a strong correlation, in either direction, for any of the stat/base stat combinations. Since a high silhouette width would mean that clusters are more precise and distinct, the result of an average silhouette width/goodness-of-fit of 0.47 can be interpreted as the structure being weak and could be artificial. In terms of the project, this would mean that there is probably not a significant difference between any of the stats and between the base stat total and any of the stats, at least in generation 1.*    
    
#### 6. Neatness, Holistic/Discretionary Points (5 pts)

- Keep things looking nice! Your project should not knit to more than 30 or so pages (probably closer to 10-20)! You will lose points if you print out your entire dataset(s), have terrible formatting, etc. If you start your project in a fresh .Rmd file, you are advised to copy set-up code below and include it: this will do things like automatically truncate if you accidentally print out a huge dataset, etc. Imagine this is a polished report you are giving to your PI or boss to summarize your work researching a topic.
